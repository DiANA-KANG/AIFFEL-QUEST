{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c886ed9",
   "metadata": {},
   "source": [
    "### 🚩 Import libraries & packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23607152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# import util tools\n",
    "import os\n",
    "from os.path import join    # define route of files\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# import data pre-processing tools\n",
    "import missingno as msno    # check missing data\n",
    "\n",
    "\n",
    "# import ML tools 1\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "# import ML tools 2\n",
    "import sklearn.ensemble as ensemble\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "    # XGBM (extreme GBM), LGBM (lighting GBM) :\n",
    "    # gradient boosting machine (GBM) 알고리즘 계열의 변형\n",
    "    # 각각 독자적인 오픈 소스 라이브러리 형태로 해당 모델을 사용할 수 있다 :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0995f3c",
   "metadata": {},
   "source": [
    "### 🚩 Define constants (hyper params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9081599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2020\n",
    "\n",
    "TEST_SIZE = 0.2    # train/test split ratio for train_test_split()\n",
    "CV_SIZE = 5        # cross validation size\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad311fc",
   "metadata": {},
   "source": [
    "### 🚩 Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f4e4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define routes of data(.csv) files\n",
    "data_dir = \"~/aiffel/kaggle_kakr_housing/data/\"\n",
    "\n",
    "\n",
    "# load csv files -> pd.DataFrame\n",
    "train_data = pd.read_csv(join(data_dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(join(data_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20be0f9",
   "metadata": {},
   "source": [
    "### 🚩 Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78630dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train data pre-processing\n",
    "\n",
    "# \"date\" column format change\n",
    "train_data[\"date\"] = train_data[\"date\"].apply(lambda i: i[:6]).astype(int)\n",
    "\n",
    "# \"price\" column regularization(?) -> grow variation of \"price\" values\n",
    "train_data[\"price\"] = np.log1p(train_data[\"price\"])\n",
    "\n",
    "# \"id\" column remove\n",
    "train_data = train_data.drop(columns = [\"id\"])\n",
    "\n",
    "\n",
    "\n",
    "### test data pre-processing\n",
    "\n",
    "# \"date\" column format change\n",
    "test_data[\"date\"] = test_data[\"date\"].apply(lambda i: i[:6]).astype(int)\n",
    "\n",
    "# \"id\" column remove\n",
    "test_data = test_data.drop(columns = [\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674f9fc",
   "metadata": {},
   "source": [
    "### 🚩 Extract feature matrices (X) & target vectors (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "557c9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data -> feature matrix (X) & target vector (y) split\n",
    "X = train_data.drop(columns = [\"price\"])    # exclude target vector column\n",
    "y = train_data[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad143dd",
   "metadata": {},
   "source": [
    "### 🚩 Define useful methods (RMSE, cross validation, grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c4dff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get RMSE losses from log(\"price\") values\n",
    "def getRMSE_log2exp(y_test, y_pred):\n",
    "    y_test, y_pred = np.expm1(y_test), np.expm1(y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "# get cross validation scores of \"one\" learning model (for model evaluation)\n",
    "def getCVscore(X, model):\n",
    "    kfold = model_selection.KFold(n_splits = CV_SIZE).get_n_splits(X.values)\n",
    "    score = np.mean(model_selection.cross_val_score(model, X = X.values, y = y, cv = kfold))\n",
    "    print(\"CV score of\", model.__class__.__name__, \":\", score)\n",
    "    return score\n",
    "        \n",
    "    \n",
    "    \n",
    "# search best parameter values for learning models\n",
    "def searchBestParams(model, X, y, param_grid, verbose = 2, n_jobs = 5):\n",
    "    # initialize grid search model\n",
    "    grid = model_selection.GridSearchCV(model, param_grid = param_grid, \\\n",
    "                                        scoring = \"neg_mean_squared_error\", \\\n",
    "                                        cv = CV_SIZE, verbose = verbose, n_jobs = n_jobs)\n",
    "    \n",
    "    # grid search model fitting\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    # return best 5 parameter values\n",
    "    result = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    result[\"score\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    result = result.sort_values(\"score\", ascending = False, ignore_index = True)\n",
    "    print(result.head())\n",
    "    return result.head()\n",
    "\n",
    "\n",
    "\n",
    "# save predicted \"price\" values as a submission file\n",
    "def makeSubmissionFile(y_pred):\n",
    "    data_dir = \"~/aiffel/kaggle_kakr_housing/data/\"\n",
    "    submission = pd.read_csv(join(data_dir, \"sample_submission.csv\"))\n",
    "    submission[\"price\"] = y_pred\n",
    "    submission.to_csv(join(data_dir, \"submission_new.csv\"), index = False)\n",
    "    print(\"The submission file has created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480221a5",
   "metadata": {},
   "source": [
    "### 🚩 Generate & evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01c9d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model instances\n",
    "extreme = xgb.XGBRegressor(random_state = RANDOM_STATE)\n",
    "light = lgb.LGBMRegressor(random_state = RANDOM_STATE)\n",
    "boost = ensemble.GradientBoostingRegressor(random_state = RANDOM_STATE)\n",
    "forest = ensemble.RandomForestRegressor(random_state = RANDOM_STATE)\n",
    "\n",
    "# create my own learning model collections\n",
    "models = [extreme, light, boost, forest]\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate model performance (cross validation)\n",
    "#for model in models:\n",
    "#    score = getCVscore(X, model)\n",
    "\n",
    "# Output :\n",
    "# CV score of XGBRegressor  :  0.8973388661281285\n",
    "# CV score of LGBMRegressor  :  0.9024911910917768\n",
    "# CV score of GradientBoostingRegressor  :  0.8796312932769542\n",
    "# CV score of RandomForestRegressor : 0.8851571351312119\n",
    "\n",
    "### It seems four models provide sufficiently high performance!\n",
    "### cross validation 실행 결과, 4개의 학습 모델이 충분한 성능을 제공한다는 것을 확인\n",
    "### 그렇다면 해당 4개의 학습 모델을 활용하여 학습 & 예측 도전!!\n",
    "### 성능 확인을 마쳤으므로, 성능 평가 과정은 주석(#)으로 처리하여 다음 코드 실행 때는 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb236fc8",
   "metadata": {},
   "source": [
    "### 🚩 Search best LGBM param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76f6b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set available values for LGBM parameters\n",
    "lgbm_param_grid = {\"max_depth\" : [-1], \\\n",
    "                    \"learning_rate\" : [0.01, 0.05, 0.1], \\\n",
    "                    \"n_estimators\" : [50, 75, 100], \\\n",
    "                    \"num_leaves\" : [26, 31, 36], \\\n",
    "                    \"boosting_type\" : [\"gbdt\"], \\\n",
    "                    \"reg_lambda\" : [30, 50, 70]}\n",
    "    # max_depth : 의사 결정 나무의 깊이, 정수 사용\n",
    "    # learning_rate : 한 스텝에 이동하는 양을 결정하는 파라미터, 보통 0.0001~0.1 사이의 실수 사용\n",
    "    # n_estimators : 사용하는 개별 모델의 개수, 보통 50~100 이상의 정수 사용\n",
    "    # num_leaves : 하나의 LightGBM 트리가 가질 수 있는 최대 잎의 수\n",
    "    # boosting_type : 부스팅 방식, gbdt, rf 등의 문자열 입력\n",
    "    # reg_lambda : L2 regularization term on weights\n",
    "\n",
    "    \n",
    "# Search best set of parameter values\n",
    "#print(searchBestParams(light, X, y, lgbm_param_grid, verbose = 0))\n",
    "\n",
    "# Output :\n",
    "#   boosting_type  learning_rate  max_depth  n_estimators  num_leaves  reg_lambda      score\n",
    "# 0          gbdt            0.1         -1           100          36          30  -0.026989\n",
    "# 1          gbdt            0.1         -1           100          31          30  -0.027051\n",
    "# 2          gbdt            0.1         -1           100          36          50  -0.027284\n",
    "# 3          gbdt            0.1         -1           100          26          30  -0.027552\n",
    "# 4          gbdt            0.1         -1           100          31          50  -0.027646\n",
    "\n",
    "### LGBM는 learning_rate = 0.1, n_estimators = 100, num_leaves = 36, reg_lambda = 30 일 때 최상의 성능임을 확인\n",
    "### XGBM 또한 LGBM과 유사한 GBM 계열의 학습 알고리즘이므로, 유사한 수치 대입하면 OK\n",
    "### 파라미터 값별 성능 확인을 마쳤으므로, 파라미터 탐색 과정은 주석(#)으로 처리하여 다음 코드 실행 때는 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb60b42",
   "metadata": {},
   "source": [
    "### 🚩 Adjust XGBD & LGBD params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9125202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate another model instances with adjusted params\n",
    "extreme = xgb.XGBRegressor(random_state = RANDOM_STATE, learning_rate = 0.2, n_estimators = 100)\n",
    "light = lgb.LGBMRegressor(random_state = RANDOM_STATE, learning_rate = 0.1, n_estimators = 300, num_leaves = 36, reg_lambda = 30)\n",
    "\n",
    "# Update XGBD & LGBD models in my model collection\n",
    "models[0] = extreme\n",
    "models[1] = light\n",
    "\n",
    "### grid search 결과 값과 유사한 값 위주로 다양한 값을 시도해 본 결과, 해당 파라미터 값으로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906f400",
   "metadata": {},
   "source": [
    "### 🚩 Perform fit() & predict() -> Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6b51e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of XGBRegressor  :  107509.3104391456\n",
      "RMSE of LGBMRegressor  :  104654.07159199048\n",
      "RMSE of GradientBoostingRegressor  :  128360.19649691365\n",
      "RMSE of RandomForestRegressor  :  125487.07102453562\n"
     ]
    }
   ],
   "source": [
    "for model in models :\n",
    "    # split train data -> for training & for valication\n",
    "    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, \\\n",
    "                                        test_size = TEST_SIZE, random_state = RANDOM_STATE)\n",
    "\n",
    "    # model fitting (learning)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict \"price\"\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    # get error (wish less than 1.1M)\n",
    "    error = getRMSE_log2exp(y_valid, y_pred)\n",
    "    print(\"RMSE of\", model.__class__.__name__, \" : \", error)\n",
    "    \n",
    "# Output :\n",
    "# RMSE of XGBRegressor  :  107509.3104391456\n",
    "# RMSE of LGBMRegressor  :  104654.07159199048\n",
    "# RMSE of GradientBoostingRegressor  :  128360.19649691365\n",
    "# RMSE of RandomForestRegressor  :  125487.07102453562\n",
    "\n",
    "### XGBM, LGBM 모델이 희망이 보이므로 두 가지 모델에 대하여 ensemble 기법을 시도해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e24ebf",
   "metadata": {},
   "source": [
    "### 🚩 Define ensemble system methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbf348e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create XGBM/LGBM models with random seed and fitting for EPOCHS iteration\n",
    "def getAveragingBlending(X, y, X_test, epochs, XGBM = False, LGBM = False):\n",
    "    y_preds = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        if XGBM:    # XGBM model\n",
    "            model = xgb.XGBRegressor(learning_rate = 0.2, n_estimators = 100)\n",
    "        else:      # LGBM model\n",
    "            model = lgb.LGBMRegressor(learning_rate = 0.1, n_estimators = 300, num_leaves = 36, reg_lambda = 30)\n",
    "            \n",
    "        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size = TEST_SIZE)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_preds.append(y_pred)  # save predicted values from each model\n",
    "\n",
    "    y_preds = np.array(y_preds) \n",
    "    mean = np.mean(y_preds, axis = 0)    # get mean values of predicted values\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdb7ed",
   "metadata": {},
   "source": [
    "### 🚩 Predict \"price\" with various learning ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27205a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. predict with only one XGBM\n",
    "#extreme = xgb.XGBRegressor(random_state = RANDOM_STATE, learning_rate = 0.2, n_estimators = 100)\n",
    "#extreme.fit(X, y)\n",
    "#y_pred = extreme.predict(test_data)\n",
    "\n",
    "\n",
    "# 2-1. predict with only one LGBM\n",
    "#light = lgb.LGBMRegressor(random_state = RANDOM_STATE, learning_rate = 0.1, n_estimators = 300, num_leaves = 36)\n",
    "#light.fit(X, y)\n",
    "#y_pred = light.predict(test_data)\n",
    "\n",
    "\n",
    "# 2-2. predict with only one LGBM & regularization parameter 30\n",
    "#light_reg = lgb.LGBMRegressor(random_state = RANDOM_STATE, learning_rate = 0.1, n_estimators = 300, num_leaves = 36, reg_lambda = 30)\n",
    "#light_reg.fit(X, y)\n",
    "#y_pred = light_reg.predict(test_data)\n",
    "\n",
    "\n",
    "# 2-3. predict with only one LGBM & regularization parameter 50\n",
    "light_reg = lgb.LGBMRegressor(random_state = RANDOM_STATE, learning_rate = 0.1, n_estimators = 300, num_leaves = 36, reg_lambda = 50)\n",
    "light_reg.fit(X, y)\n",
    "y_pred = light_reg.predict(test_data)\n",
    "\n",
    "\n",
    "# 3. predict with ensembled XGBMs\n",
    "#y_pred = getAveragingBlending(X, y, test_data, EPOCHS, XGBM = True)\n",
    "\n",
    "\n",
    "# 4. predict with ensembled LGBMs\n",
    "#y_pred = getAveragingBlending(X_train, y_train, X_valid, EPOCHS, LGBM = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc751c",
   "metadata": {},
   "source": [
    "### 🚩 Save predicted \"price\" as submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93c3aa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The submission file has created succesfully.\n"
     ]
    }
   ],
   "source": [
    "# recover original price value range\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "# save as file\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479bcee",
   "metadata": {},
   "source": [
    "-----\n",
    "**회고록** :  \n",
    "XGBM, LGBM 이 다른 학습 모델에 비해 성능이 좋다던데, 몸소 체험할 수 있었다  \n",
    "또한 같은 모델이라도 parameter 값에 따라서도 충분히 성능을 조정할 수 있음을 체감했다  \n",
    "동일한 모델에 서로 다른 random seed를 부여해서 ensemble 을 시도하였는데, 대부분 성능이 좋지 않았다\n",
    "서로 다른 모델들을 묶어서 ensemble 을 해야 성능이 향상되는걸까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1fab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
